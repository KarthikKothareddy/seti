{"cells":[{"metadata":{"trusted":true,"_uuid":"f21ce55e4ccc7b9e07542e34b036fcc4f55e3697"},"cell_type":"code","source":"import os\nimport cv2\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2ce0fa5691f0537ec1891cdf7e634be65d5edb4"},"cell_type":"markdown","source":"## SETI Explanatory Analysis and Preprocessing Images\n\nLet's start by examining images in each class, below snippet will randomly select two images from the training data and displays them."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"04acbcb43b675afc4cace19a4219d48d27d196ce"},"cell_type":"code","source":"# all classes \nclasses = [\"brightpixel\",\n            \"narrowband\",\n            \"narrowbanddrd\",\n            \"noise\",\n            \"squarepulsednarrowband\",\n            \"squiggle\",\n            \"squigglesquarepulsednarrowband\"]\nnum_images = 2\nfor _class in classes:\n    # start off by observing images\n    path = os.path.join(\"../input/primary_small/train\", _class)\n    image_files = os.listdir(path)\n    random_images = random.sample(range(0, len(image_files)-1), num_images)\n    fig, axes = plt.subplots(nrows=1, ncols=num_images, figsize=(12, 14), squeeze=False)\n    fig.tight_layout()\n    for l in range(1):\n        for m in range(num_images):\n            axes[l][m].imshow(cv2.imread(os.path.join(path, image_files[random_images[m]]), 0), cmap=\"gray\")\n            axes[l][m].axis(\"off\")\n            axes[l][m].set_title(_class)\n# done displaying","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0792a4986e3ecb738bdd1ec7036317604e39562"},"cell_type":"markdown","source":"One can observe that despite many signals being strong with less noice, we still have some images with high level of noice and even some signals hiding along noice. So we'll do some preprocessing to the images to see if that can extract the features we're looking for.\n\nLet's pick a random image from \"narrowband\" class and perform some pre-processing on them and see how it turns out. Before that let's construct a helper method that takes a numpy array and displays it as a image."},{"metadata":{"trusted":true,"_uuid":"3ed55a531a5d522ae927b5439bd2c064a8f31c5d"},"cell_type":"code","source":"def display(image):\n    fig = plt.figure(figsize=(9, 11))\n    plt.imshow(image, cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6edb0d363fc5c8a1c49f8df60277b0c14613dbb2"},"cell_type":"code","source":"_random = random.choice(os.listdir(os.path.join(\"../input/primary_small/train/narrowband\")))\n# lets read the image and display it\nnarrowband = cv2.imread(os.path.join(\"../input/primary_small/train/narrowband\", _random))\ndisplay(narrowband)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fc23cce6dffdfcb5a59db071b6d325e9cd729b2"},"cell_type":"markdown","source":"Notice how the image is displayed in weird colorspace? This is because openCV reads an image in \"BGR\" format when compared to matplotlib expectation of \"RGB\". Let's make the image grayscale and start applying preprocessing."},{"metadata":{"trusted":true,"_uuid":"aa631bcae2184e51859f1ba8741d57e123fd8814"},"cell_type":"code","source":"# convert from BGR to Grayscale\nnarrowband = cv2.cvtColor(narrowband, cv2.COLOR_BGR2GRAY)\ndisplay(narrowband)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3311129fdde0ecae1adffd6e2885df9b624c61a"},"cell_type":"markdown","source":"After inspecting from the below features, we can clearly see that there are outliers in our images which are far away from the mean"},{"metadata":{"trusted":true,"_uuid":"50ae721850cb8e11921494ac7c1181766d67db97"},"cell_type":"code","source":"# now let's extract some features from the image\nlow = np.min(narrowband)\nhigh = np.max(narrowband)\nmean = np.mean(narrowband)\nstd = np.std(narrowband)\nvariance = np.var(narrowband)\n# print\nprint(\"Min: {}\".format(low))\nprint(\"Max: {}\".format(high))\nprint(\"Mean: {}\".format(mean))\nprint(\"Standard Deviation: {}\".format(std))\nprint(\"Variance: {}\".format(variance))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e940108793a7a031482ce7691cf94a597237454"},"cell_type":"markdown","source":"Let's make an assumption that every pixel which is 3.5 times standard deviations away from mean value is an outlier and clip it's value to the 3.5 * std"},{"metadata":{"trusted":true,"_uuid":"9d9f54c89e2d8ab6c9b13b9df7156caac9a3da35"},"cell_type":"code","source":"clipped = np.clip(narrowband, mean-3.5*std, mean+3.5*std)\n# print\nprint(\"Min: {}\".format(np.min(clipped)))\nprint(\"Max: {}\".format(np.max(clipped)))\ndisplay(clipped)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38d27668af738a87a5443d683037150f88abf0eb"},"cell_type":"markdown","source":"We can see that the maximum pixel intensity reduced by clipping the image which makes much smoother image. Now let's perform some image arithmetic methods like applying Gaussian Blurr, Morphing and Gradient selection"},{"metadata":{"trusted":true,"_uuid":"fc3ae7fbef99836f3d8850c78e0ed2e5caad37a7"},"cell_type":"code","source":"# Gaussian blurr\ngaussian = cv2.GaussianBlur(narrowband, (3, 3), 1)\nprint(\"Min: {}\".format(np.min(gaussian)))\nprint(\"Max: {}\".format(np.max(gaussian)))\nprint(\"Mean: {}\".format(np.mean(gaussian)))\ndisplay(gaussian)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bf1c88b093f0069418127c8e79e36c96c90b6e8"},"cell_type":"markdown","source":"The Gaussian Blurr increased the min and reduced the max pixel intensities further for the image packing the pixels closer to each other that smoothens the image. Unofrtunately for images that have weak signals Gaussian blurr actually blurrs part of the signal itself which we may not help, so it's upto us on if we want to use the Gaussian blurr or not(I decided to use it for further downstream transformations). \n\nLet's move on to morphological operations on the narrowband image."},{"metadata":{"trusted":true,"_uuid":"fae35a45f8b19b4f6f505935d02cc8cbc0ca0a7a"},"cell_type":"code","source":"# lets do a morphological closing on the clipped image which is dilation + erosion\nmorphed = cv2.morphologyEx(gaussian, cv2.MORPH_CLOSE, kernel=np.ones((3, 3), dtype=np.float32))\ndisplay(morphed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35624390b0e71293def22aff265a0187357ac584"},"cell_type":"markdown","source":"This morphological closing clearly helped some of the low intensity signal pixels to expose more and brighten the signal compared to background noice. \n\nNow, lets observe all the images from all classes (scroll above to 3rd cell) and we can notice that each signal has some gradient along the horizontal axis and we'll make use of that gradient by applying Sobel operations to the morphed image"},{"metadata":{"trusted":true,"_uuid":"7b861353a8f1cee1b87f4379bfa04c9315f3d7e6"},"cell_type":"code","source":"# we'll start by applying sobel edge detection along x-axis\nsobelx = cv2.Sobel(morphed, cv2.CV_64F, 1, 0, 2)\ndisplay(sobelx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb2d4ee4639a96103e556fd2c2a74e212b983022"},"cell_type":"code","source":"# let's apply sobel ege detection along y-axis\nsobely = cv2.Sobel(morphed, cv2.CV_64F, 0, 1, 2)\ndisplay(sobely)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6881704f833f39c00d81882f3733fcb6fc4f693"},"cell_type":"markdown","source":"Now, let's blend both images with more horizontal weight as we know all signals have a gradient along horizontal axis"},{"metadata":{"trusted":true,"_uuid":"892dd533b89dd486dc711a2cf3b55b7bbfd109e4"},"cell_type":"code","source":"blended = cv2.addWeighted(src1=sobelx, alpha=0.7, src2=sobely, beta=0.3, gamma=0)\ndisplay(blended)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1c1af595dcc80d6389b33b758a1439a618a4192"},"cell_type":"markdown","source":"The above image is so far the smoothest image and it clearly distinguishes the signal from background which will save a lot of time later when classifying them via a Convolutional Neural Network.\n\nFinally let's apply same set of transformations to couple of images in each class and see if the results are uniform. Before that let's copy all the above transformations into a method."},{"metadata":{"trusted":true,"_uuid":"6ff416265208e1c265fb731b9ed3255aa85c7384"},"cell_type":"code","source":"def process_image(image):\n    # grayscale conversion\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    # clip intensities\n    mean = np.mean(image)\n    std = np.std(image)\n    image = np.clip(image, mean-3.5*std, mean+3.5*std)\n    # morph close \n    morphed = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel=np.ones((3, 3), dtype=np.float32))\n    # gradient in both directions\n    sobelx = cv2.Sobel(morphed, cv2.CV_64F, 1, 0, 2)\n    sobely = cv2.Sobel(morphed, cv2.CV_64F, 0, 1, 2)\n    # blend \n    blended = cv2.addWeighted(src1=sobelx, alpha=0.7, src2=sobely, beta=0.3, gamma=0)\n    return blended\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"daa36b72575347bf33f0c0b51b4d9dc60ef1c9ef"},"cell_type":"code","source":"for _class in classes:\n    # start off by observing images\n    path = os.path.join(\"../input/primary_small/train\", _class)\n    image_files = os.listdir(path)\n    random_images = random.sample(range(0, len(image_files)-1), num_images)\n    fig, axes = plt.subplots(nrows=1, ncols=num_images, figsize=(11, 12), squeeze=False)\n    fig.tight_layout()\n    for l in range(1):\n        for m in range(num_images):\n            axes[l][m].imshow(process_image(cv2.imread(os.path.join(path, image_files[random_images[m]]))), cmap=\"gray\")\n            axes[l][m].axis(\"off\")\n            axes[l][m].set_title(_class)\n# done displaying","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f98954e45c8d3283c8e83165c71c5f312c03f83a"},"cell_type":"markdown","source":"As you see that the above pre-processign algorithm did really good in highlighting the signal in each class,even for the classes that have some vertical gradient in them such as Squiggle and Squigglesquarepulsednarrowband by preserving their shape.\n\nWe can pre-process the images further by applying Image PCA, Dialtion and other image arithmetic techniques, but these should be sufficient to feed the images to any Machine Learning/Deep Learning algorithms for successful classification."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}